<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Broca's Brain (neural networks)</title><link>https://jaidevd.github.io/</link><description></description><atom:link href="https://jaidevd.github.io/categories/neural-networks.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Wed, 05 Jul 2017 05:22:17 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>A Geometric Proof of the Perceptron Convergence Theorem</title><link>https://jaidevd.github.io/posts/a-geometric-proof-of-the-perceptron-convergence-theorem/</link><dc:creator>Jaidev Deshpande</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The last time I studied neural networks in detail was five years ago in college. I did touch upon backpropagation when Andrew Ng's machine learning MOOC was offered on Coursera for the first time, but beyond that I've only dabbled with them through &lt;a href="http://keras.io"&gt;keras&lt;/a&gt;. Then recently, when I read about Coursera's imminent decision to pull down much of their freely available material (you can read a rant about it &lt;a href="http://reachtarunhere.github.io/2016/06/11/Golden-Age-of-MOOCs-is-over-and-why-I-hate-Coursera/"&gt;here&lt;/a&gt;), I went on a downloading spree (many thanks to the wonderful &lt;a href="http://github.com/coursera-dl/coursera-dl"&gt;coursera-dl&lt;/a&gt;). Of all the courses I downloaded, the one that caught my eye was Geoffrey Hinton's course on &lt;em&gt;Neural Networks for Machine Learning&lt;/em&gt;. Because of that and the fact that there were some computer vision projects going on at work, I decided to dive right in.&lt;/p&gt;
&lt;p&gt;Hinton's course is wonderful. He is funny, and unsurprisingly, very very insightful about the core concepts in neural networks. One of the signs of this is the fact that this course is not at all cluttered with too much mathematics, and can be traveresed by someone with only a working knowledge of calculus. One of his most insightful moments in the course is when he describes the Perceptron learning rule as simply as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the perceptron makes no mistake, leave it alone.&lt;/li&gt;
&lt;li&gt;If it predicts a false negative, add the input vector to the weight vector&lt;/li&gt;
&lt;li&gt;If it predicts a false positive, subtract the input vector from the weight vector&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is so simple, that a literal implementation of this can make train a perceptron reasonably well (as we shall see). There are of course, numerous heuristics required when applying it in production, but the training algorithm is just this simple. Now, the popularity of the perceptron is because it guarantees linear convergence, i.e. if a binary classification problem is linearly separable in the feature space, the perceptron will &lt;em&gt;always eventually&lt;/em&gt; learn to correctly classify the input samples. An algebraic or analytical proof of this can be found anywhere, but relies almost always on the &lt;a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality"&gt;Cauchy-Schwarz inequality&lt;/a&gt;. I thought that since the learning rule is so simple, then there must be a way to understand the convergence theorem using nothing more than the learning rule itself, and some simple data visualization. I think I've found a reasonable explanation, which is what this post is broadly about. But first, let's see a simple demonstration of training a perceptron.
&lt;/p&gt;&lt;p&gt;&lt;a href="https://jaidevd.github.io/posts/a-geometric-proof-of-the-perceptron-convergence-theorem/"&gt;Read moreâ€¦&lt;/a&gt; (9 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>machine learning</category><category>math</category><category>neural networks</category><category>python</category><guid>https://jaidevd.github.io/posts/a-geometric-proof-of-the-perceptron-convergence-theorem/</guid><pubDate>Wed, 06 Jul 2016 09:15:55 GMT</pubDate></item></channel></rss>